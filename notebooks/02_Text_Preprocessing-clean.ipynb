{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patent Corpus Text Cleaning\n",
    "\n",
    "* Patent are a special kind of textual data that contain plenty of technical terms, specific words serving as transition phrases, and numerous academic words that describe invention outcomes. Before modeling topics with LDA, we also utilize three modules to remove general words from the corpus of patents as follows:\n",
    "    * Stop words such as the, that, and these;\n",
    "    * High-frequency words in patent claims such as claimed, comprising, and invention;\n",
    "    * General academic words such as research, approach, and data.\n",
    "\n",
    "**Among all the terms only technical terms provide the most meaningful information that reflects technological topics and innovations.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "os.chdir('/Users/sheeroh/Box Sync/2_projects/insightDSNYC/data/')\n",
    "from loadData_workflow import loadData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the saved pickle file patent document\n",
    "patdocs = pd.read_pickle( 'patdocs_clean.pkl')\n",
    "\n",
    "#check if all data has been successfully loaded\n",
    "#patdocs.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Text Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "def patent_to_words( raw_review ):\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_review) \n",
    "    lower_case = letters_only.lower()        # Convert to lower case\n",
    "    words = lower_case.split()               # Split into words\n",
    "    from nltk.corpus import stopwords # Import the stop word list\n",
    "    #print (stopwords.words(\"english\"))\n",
    "    words = [w for w in words if not w in stopwords.words(\"english\")] \n",
    "    #print (words)\n",
    "    xx = stopwords.words(\"english\")\n",
    "    # Add first, second and one\n",
    "    xx.extend([\"first\",\"second\",\"one\",\"two\",\"also\",\"may\",\"least\",\"present\",\"determine\",\n",
    "    \"included\",\"includes\",\"include\",\"provided\",\"provides\",\"wherein\",\"method\",\"methods\",\n",
    "    \"comprises\",\"comprised\",\"comprising\",\"used\",\"uses\",\"using\",\"use\",\"say\",\"says\",\"said\",\"disclose\",\"discloses\",\"disclosed\",\n",
    "    \"containing\",\"contain\",\"contains\",\"contained\",\"make\",\"made\",\"makes\",\"end\",\"couple\",\"relates\", 'invention','including',\n",
    "    \"b\",\"c\",\"d\", 'new','described', 'gift', 'A', \"research\", 'group', 'according',\"approach\", 'data', 'system', 'x', 'claimed', 'claim', 'therein'])\n",
    "    stops = set(xx)               \n",
    "    #Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    return( \" \".join( meaningful_words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_to_words(patdocs[\"abstract\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and Tokenize patents into lists (each patent is a words array)\n",
    "#Saved clean_abstract to file so as to pick it up from here later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Get the number of patents based on the dataframe column size\n",
    "num_patents = patdocs[\"abstract\"].size\n",
    "print(num_patents)\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_abstract = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length of the patent list \n",
    "for i in range( 0, num_patents ):\n",
    "    patent = patent_to_words(patdocs[\"abstract\"][i])\n",
    "    if i%10000==0:print(i)\n",
    "    array = patent.split()\n",
    "    clean_abstract.append(array)\n",
    "with open('clean_abstract.txt', 'w') as outfile:\n",
    "    json.dump(clean_abstract, outfile)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, using gensim's Phrases function, we create additional bigrams to include in the topic modeling. We used [12] as a reference to create the bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.corpora import Dictionary\n",
    "# Identify Bigrams using gensim's Phrases function\n",
    "\n",
    "#bigram = Phraser(phrases)\n",
    "bigram = models.Phrases(clean_abstract, delimiter=b'_')\n",
    "print(bigram)\n",
    "\n",
    "final_abstract = []\n",
    "for i in range(0,num_patents):\n",
    "    sent = clean_abstract[i] \n",
    "    temp_bigram = bigram[sent]\n",
    "    final_abstract.append(temp_bigram)\n",
    "    #if i%10000==0:print(i)\n",
    "final_column = pd.Series(final_abstract)\n",
    "   \n",
    "#list(bigram[final_abstract])\n",
    "#print(bigram[sent])\n",
    "#len(bigram.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column final_column\n",
    "final_column = pd.Series(final_abstract)\n",
    "\n",
    "#print(final_column)\n",
    "patdocs['final_column']= final_column.values\n",
    "patdocs.head(2)\n",
    "patdocs.to_csv('patdocs_final_column.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert tokenized document to dictionary and document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"model\")\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(final_abstract)\n",
    "dictionary.save('abstract_new.dict') # store the dictionary, for future reference-\n",
    "     \n",
    "# convert tokenized documents into a document-term matrix (bag-of-words)\n",
    "corpus = [dictionary.doc2bow(text) for text in final_abstract]\n",
    "corpora.MmCorpus.serialize('abstract_new.mm', corpus)\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))\n",
    "\n",
    "#print(corpus)\n",
    "#mm_corpus = gensim.corpora.MmCorpus('abstract.mm')\n",
    "#print(mm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_abstract)-1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "random.seed(7)# 42 is not always th2 answer, let's try something different :)\n",
    "\n",
    "train_set = random.sample(list(range(0,len(final_abstract))),len(final_abstract)-1000)\n",
    "test_set = [x for x in list(range(0,len(final_abstract))) if x not in train_set]\n",
    "\n",
    "train_texts = [final_abstract[i] for i in train_set]\n",
    "test_texts = [final_abstract[i] for i in test_set]\n",
    "\n",
    "pickle.dump([train_set,test_set,train_texts,test_texts],open('./abstract_train_test_sets_new.pkl','wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
