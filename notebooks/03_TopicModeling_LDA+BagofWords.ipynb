{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA Model Fitting and Analysis\n",
    "* Fit an LDA model to a corpus of patent claims_abstract\n",
    "* Identify the topics discovered by the model in terms of their most important words, and we want to use the model to predict the topic probability distribution for a given document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "# Plotting libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.corpora import Dictionary\n",
    "os.chdir('/Users/sheeroh/Box Sync/2_projects/insightDSNYC/data/model')\n",
    "#print (\"We load our dictionary : %s\"% type(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the saved pickle file patent document\n",
    "patdocs = pd.read_pickle( '../patdocs_clean.pkl')\n",
    "\n",
    "#check if all data has been successfully loaded\n",
    "#patdocs.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_patents = patdocs[\"abstract\"].size\n",
    "print(num_patents)\n",
    "\n",
    "with open('../clean_abstract.txt', 'r') as infile:\n",
    "    clean_abstract=json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "# Identify Bigrams using gensim's Phrases function\n",
    "\n",
    "#bigram = Phraser(phrases)\n",
    "bigram = models.Phrases(clean_abstract, delimiter=b'_')\n",
    "print(bigram)\n",
    "\n",
    "final_abstract = []\n",
    "for i in range(0,num_patents):\n",
    "    sent = clean_abstract[i] \n",
    "    temp_bigram = bigram[sent]\n",
    "    final_abstract.append(temp_bigram)\n",
    "    #if i%10000==0:print(i)\n",
    "final_column = pd.Series(final_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_column = pd.Series(final_abstract)\n",
    "#print(final_column)\n",
    "patdocs['final_column']= final_column.values\n",
    "#patdocs.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "random.seed(7)# 42 is not always teh answer, let's try something different :)\n",
    "train_set = random.sample(list(range(0,len(final_abstract))),len(final_abstract)-1000)\n",
    "test_set = [x for x in list(range(0,len(final_abstract))) if x not in train_set]\n",
    "\n",
    "train_texts = [final_abstract[i] for i in train_set]\n",
    "test_texts = [final_abstract[i] for i in test_set]\n",
    "\n",
    "pickle.dump([train_set,test_set,train_texts,test_texts],open('../pat_abstract_train_test_sets_new.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Jaccard Coefficient to Determine Number of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy this for JS\n",
    "def jaccard_similarity(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return float(len(intersection))/float(len(union))\n",
    "\n",
    "topicnums = [1,5,10,15,20,30,40,50]\n",
    "dictionary = corpora.Dictionary(train_texts)\n",
    "pickle.dump(dictionary,open('./abstract_ldamodels_bow_dictionary_new.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "corpus = [dictionary.doc2bow(text) for text in train_texts]\n",
    "\n",
    "ldamodels_bow = {}\n",
    "for i in topicnums:\n",
    "    random.seed(42)\n",
    "    %time ldamodels_bow[i] = models.ldamodel.LdaModel(corpus,num_topics=i,id2word=dictionary)\n",
    "    ldamodels_bow[i].save('./ldamodels_bow_'+str(i)+'_new.lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the words in topics to determine a good number of topics to use\n",
    "topicnums = [1,5,10,15,20,30,40,50]\n",
    "\n",
    "lda_topics = {}\n",
    "for i in topicnums:\n",
    "    lda_model = models.ldamodel.LdaModel.load('./ldamodels_bow_'+str(i)+'_new.lda')\n",
    "    lda_topics_string = lda_model.show_topics(i)\n",
    "    lda_topics[i] = [\"\".join([c if c.isalpha() else \" \" for c in topic[1]]).split() for topic in lda_topics_string]\n",
    "\n",
    "pickle.dump(lda_topics,open('./abstract_lda_bow_topics_new.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use JS to find similarity between topics- Copy the \n",
    "lda_stability = {}\n",
    "for i in range(0,len(topicnums)-1):\n",
    "    jacc_sims = []\n",
    "    for t1,topic1 in enumerate(lda_topics[topicnums[i]]):\n",
    "        sims = []\n",
    "        for t2,topic2 in enumerate(lda_topics[topicnums[i+1]]):\n",
    "            sims.append(jaccard_similarity(topic1,topic2))    \n",
    "        jacc_sims.append(sims)    \n",
    "    lda_stability[topicnums[i]] = jacc_sims\n",
    "    \n",
    "pickle.dump(lda_stability,open('./abstract_lda_bow_stability.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# visualization\n",
    "import seaborn as sns \n",
    "plt.style.use('fivethirtyeight')\n",
    "#sns.set_style(\"white\")\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8,4) \n",
    "#plt.rcParams['axes.titlesize'] = 'large'\n",
    "topicnums = [1,5,10,20,30,40]\n",
    "\n",
    "#lda_stability = pickle.load(open('./abstract_lda_bow_stability_new.pkl','rb'))\n",
    "mean_stability = [np.array(lda_stability[i]).mean() for i in topicnums]\n",
    "\n",
    "with sns.axes_style(\"white\"):\n",
    "    x = topicnums\n",
    "    y = mean_stability\n",
    "    plt.plot(x,y,label='Average Overlap Between Topics')\n",
    "    plt.xlim([0, 30])\n",
    "    plt.ylim([0, 0.3])\n",
    "    plt.xlabel('Number of topics')\n",
    "    plt.ylabel('Average Jaccard similarity')   \n",
    "    plt.title('Average Jaccard Similarity Between Topics')\n",
    "    plt.legend()    \n",
    "    plt.show()\n",
    "    # from the plot select number of optimal number of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Model - Load optimal # of topics into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reload the lda model \n",
    "import pickle\n",
    "num_topics = 10\n",
    "lda_model = models.ldamodel.LdaModel.load('./ldamodels_bow_'+str(num_topics)+'_new.lda')\n",
    "doc_dict = pickle.load(open('./abstract_ldamodels_bow_dictionary_new.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out the top words in each topic and dump it into a pickle file. \n",
    "lda_topics = lda_model.show_topics(num_topics)\n",
    "lda_topics_words = [\"\".join([c if c.isalpha() else \" \" for c in topic[1]]).split() for topic in lda_topics]\n",
    "lda_topics_disp = [(\"topic \"+str(i)+\": \")+\" \".join(topic) for i,topic in enumerate(lda_topics_words)]\n",
    "print(lda_topics_disp)\n",
    "#already saved\n",
    "#pickle.dump(lda_topics_disp,open('./abstract_lda_bow_topics_new.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### group topic extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#patdocs.drop(['Unnamed: 0'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = pickle.load(open('./abstract_ldamodels_bow_dictionary_new.pkl','rb'))\n",
    "group_doc = {2010:[], 2011:[], 2012:[], 2013:[], 2014:[], 2015:[]}\n",
    "group_doc[2010] = patdocs.loc[patdocs['year'] ==2010]['final_column'].tolist()\n",
    "group_doc[2011] = patdocs.loc[patdocs['year'] ==2011]['final_column'].tolist()\n",
    "group_doc[2012] = patdocs.loc[patdocs['year'] ==2012]['final_column'].tolist()\n",
    "group_doc[2013] = patdocs.loc[patdocs['year'] ==2013]['final_column'].tolist()\n",
    "group_doc[2014] = patdocs.loc[patdocs['year'] ==2014]['final_column'].tolist()\n",
    "group_doc[2015] = patdocs.loc[patdocs['year'] ==2015]['final_column'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic vectors\n",
    "group_topics = {}\n",
    "for i in group_doc.keys():\n",
    "    doc_corp = dictionary.doc2bow(group_doc[i][0])\n",
    "    doc_prob = lda_model[doc_corp]\n",
    "    #print(doc_prob)\n",
    "    \n",
    "    topic_prob = [0 for j in range(num_topics)]\n",
    "    for prob in doc_prob:\n",
    "        topic_prob[prob[0]] = prob[1]\n",
    "    group_topics[i] = topic_prob\n",
    "\n",
    "#pickle.dump(group_topics, open('./abstract_lda_bow_topics_new2.pkl','wb')) \n",
    "print(group_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_topics = {}\n",
    "for i in range(num_topics):\n",
    "    y_topics[i] = []\n",
    "    for j in group_doc.keys():\n",
    "        y_topics[i].append(group_topics[j][i])\n",
    "print(y_topics)\n",
    "x = [ 2010, 2011, 2012, 2013, 2014, 2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_topics):\n",
    "    plt.plot(x, y_topics[i], label=\"topic \"+str(i))\n",
    "plt.legend()\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Probability distribution')   \n",
    "plt.title('Topic Variation from years 2010-2015')\n",
    "plt.xticks(x, x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_topics):\n",
    "    print (lda_topics_disp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_topics[1])\n",
    "print (lda_topics_words[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization- Save this topic distribution in a format that D3plus visualizations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next ,we will have to save this topic distribution in a format that D3plus visualizations can read. \n",
    " * This is a JSON file with an array structure. \n",
    " * Each element of the array is a dictionary. \n",
    " * Each of these dictionaries hold exactly exactly the same set of keys, but they have\n",
    " different values.\n",
    " \n",
    "First, we will need to create a shorter set of topic descriptors, since the default 10 words per topic is too much. We have this configruation into the lda_topics_disp variable, as well as create a new one with only the first 4 words per topic, under lda_topics_disp2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topics = lda_model.show_topics(num_topics)\n",
    "lda_topics_words = [\"\".join([c if c.isalpha() else \" \" for c in topic[1]]).split() for topic in lda_topics]\n",
    "lda_topics_disp = [(\"topic \"+str(i)+\": \")+\" \".join(topic) for i,topic in enumerate(lda_topics_words)]\n",
    "n=4\n",
    "lda_topics_words2 = [\"\".join([c if c.isalpha() else \" \" for c in topic[1]]).split()[:n] for topic in lda_topics]\n",
    "lda_topics_disp2 = [\" \".join(topic) for i,topic in enumerate(lda_topics_words2)]\n",
    "lda_topics_disp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (lda_topics_words2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (lda_topics_words[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a python array, data and push the values into it. \n",
    "\n",
    "*Remember the D3plus format requirements? Each array item has to be a dictionary with the same set of key-value pairs. \n",
    "* Here we use the y_topics variable we created previously and cycle through all topics and all years. \n",
    "* Each topic-year combination will yield a unique value for the proportion of that certain topic in that particular year - and therefore an entry in our data vector.\n",
    "\n",
    "We store the year values under the year key, and the values under the value key. Furthermore, we store the numerical value of the topic under topic_id, as well as two version of the descriptive topic labels, the 10-word version under the key topic_name, and the 4-word version under topic_name2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/Users/sheeroh/Box Sync/2_projects/insightDSNYC/data/d3plus')\n",
    "data=[]\n",
    "for topic in range(len(y_topics)):\n",
    "    for year in range(len(x)):\n",
    "        data.append({\"year\":x[year],\n",
    "                     \"value\":y_topics[topic][year],\n",
    "                     \"topic_id\":topic,\"topic_name\":lda_topics_disp[topic],\n",
    "                     \"topic_name2\":lda_topics_disp2[topic]})\n",
    "with open('d3plus'+str(num_topics)+'.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some questions??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. *NJ*: I see the files are generated, but when i try to visualize through the server, it doesn't work. and I am not sure what I am doing wrong\n",
    "\n",
    "2. Why gensim as opposed to scikit-learn? I thnk of gensim as being used for word2vec, but apparently there is an LDA module for it, too-- I didn't know that!\n",
    "Why LDA? (as opposed to NMF, for example?)\n",
    "\n",
    "3. Did you try others and this gave you the best result? \n",
    "4. How did you model perform on a hold-out (test) set of patents? Was it able to assign topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
